import argparse
import pathlib
import math
from collections import defaultdict
from typing import Dict, List, Tuple

def load_tfidf_vectors(tfidf_dir: pathlib.Path) -> Tuple[Dict[str, Dict[str, float]], set]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç TF-IDF-–≤–µ–∫—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - —Å–ª–æ–≤–∞—Ä—å: {stem: {term: tfidf}}
      - –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Ç–µ—Ä–º–æ–≤
    """
    vectors = {}
    vocabulary = set()

    for path in tfidf_dir.glob("*.txt"):
        stem = path.stem.replace("_tokens_tf_idf", "").replace("_lemmas_tf_idf", "")
        with path.open(encoding="utf-8") as f:
            tfidf_vector = {}
            for line in f:
                term, _, tfidf = line.strip().split()
                tfidf = float(tfidf)
                tfidf_vector[term] = tfidf
                vocabulary.add(term)
            vectors[stem] = tfidf_vector

    return vectors, vocabulary


def build_query_vector(query_terms: List[str], doc_vectors: Dict[str, Dict[str, float]]) -> Dict[str, float]:
    """
    –°—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞: tf = 1, idf –±–µ—Ä—ë—Ç—Å—è –∏–∑ –ª—é–±–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –≥–¥–µ –µ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω.
    """
    query_vector = {}
    doc_count = len(doc_vectors)
    df = defaultdict(int)

    for term in query_terms:
        for vec in doc_vectors.values():
            if term in vec:
                df[term] += 1

    for term in query_terms:
        if df[term] > 0:
            idf = math.log(doc_count / df[term])
            query_vector[term] = idf  # tf=1

    return query_vector


def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    common_terms = set(vec1.keys()) & set(vec2.keys())
    numerator = sum(vec1[t] * vec2[t] for t in common_terms)

    norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))
    norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return numerator / (norm1 * norm2)


def search(query: str, tfidf_dir: pathlib.Path, use_lemmas: bool = False, top_k: int = 5):
    vectors, _ = load_tfidf_vectors(tfidf_dir)
    query_terms = query.lower().split()
    query_vec = build_query_vector(query_terms, vectors)

    scores = []
    for stem, vec in vectors.items():
        score = cosine_similarity(query_vec, vec)
        if score > 0:
            scores.append((stem, score))

    scores.sort(key=lambda x: x[1], reverse=True)

    print(f"\nüîç Top {top_k} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    for rank, (stem, score) in enumerate(scores[:top_k], start=1):
        print(f"{rank}. {stem} ‚Äî score: {score:.4f}")


def main():
    parser = argparse.ArgumentParser(description="–ü–æ–∏—Å–∫ –ø–æ TF-IDF –∏–Ω–¥–µ–∫—Å–∞–º")
    parser.add_argument("query", help="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
    parser.add_argument("--tfidf-dir", default="tokens_tf_idf", type=pathlib.Path,
                        help="–ö–∞—Ç–∞–ª–æ–≥ —Å TF-IDF —Ñ–∞–π–ª–∞–º–∏")
    parser.add_argument("--use-lemmas", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–µ–º–º—ã –≤–º–µ—Å—Ç–æ —Ç–µ—Ä–º–∏–Ω–æ–≤")
    parser.add_argument("--top-k", type=int, default=10, help="–°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–≤–æ–¥–∏—Ç—å")
    args = parser.parse_args()

    tfidf_dir = pathlib.Path("lemmas_tf_idf" if args.use_lemmas else "tokens_tf_idf")
    search(args.query, tfidf_dir=tfidf_dir, top_k=args.top_k)


if __name__ == "__main__":
    main()
