# Задачи курса "Основы информационного поиска"

## Выполнили: [Торопов Дмитрий Александрович](https://github.com/DeM0NChiCk) и [Чивилев Никита Сергеевич](https://github.com/nVoxel)

### Задание №1

#### Требования

- создать краулер
 
#### Результат

- links.zip: архив с выкаченными страницами в формате html
- index.txt: текстовый файл в котором хранятся номера файлов и ссылки на страницы

#### Инструменты

- Python 3.12 
- beautifulsoup4 4.13.3  

Beautiful Soup — это библиотека, которая упрощает сбор информации с веб-страниц. Она располагается поверх HTML- или XML-парсера, предоставляя Pythonic-идиомы для итерации, поиска и изменения дерева парсинга.

#### Ссылки для краулера (последняя дата обращения 18.03.2025)

Научно-популярные статьи рубрики: Исследования
- https://www.trv-science.ru/category/news/
- https://www.trv-science.ru/category/news/path/2

#### Запуск

- Установите необходимые пакеты: `pip install beautifulsoup4`
- `python crawler.py`

### Задание №2

#### Требования

- разбить текст на отдельные слова (токены) на кириллице
- отфильтровать токены
- лемматизировать оставшиеся токены

#### Входные данные

- папка links, в которой лежат HTML-файлы вида unloading_N.html, где N — целое число
- каждый HTML-файл должен содержать блок \<div id="main"> и внутри него хотя бы один дочерний \<div>, откуда будет браться текст

#### Результат
Для каждого unloading_N.html создаются два файла:
- tokens_unloading_N.txt: cписок токенов (по одному в каждой строке)
- lemmas_unloading_N.txt: cписок лемм и соответствующих им форм в виде

#### Инструменты
- Python 3.10 (pymorphy не работает на Python 3.12)
- beautifulsoup4 4.13.3 - для парсинга HTML-страниц и удобного извлечения нужных элементов из DOM
- nltk 3.9.1 - для токенизации и работы со стоп-словами
- pymorphy2 0.9.1 - морфологический анализатор русского языка, позволяет определять леммы

#### Запуск
- Установите необходимые пакеты: `pip install beautifulsoup4 nltk pymorphy2`
- Скачайте необходимые данные NLTK (стоп-слова, punkt), если ещё не скачивали:
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```
- `python tokenizer.py`

### Задание №3

#### Требования

- создать инвертированный список терминов (индекс)
- реализовать булев поиск по построенному индексу
- Замечание:
  - в рамках выполнения задания должны быть реализованы операторы AND, OR, NOT
  - возможность вводить сложный запрос ( например, (Клеопатра  AND Цезарь) OR (Антоний AND Цицерон) OR Помпей
  - реализована возможность водить  запрос  строку запроса (то есть запрос не "хардкодим")

#### Входные данные

- папка links, в которой лежат HTML-файлы вида unloading_N.html, где N — целое число
- папка lemmas, в которой лежат файлы lemmas_unloading_N.txt: cписок лемм и соответствующих им форм в виде

#### Результат

Создаётся файл c индексами (инвертированный список терминов)
- inverted_index.txt: файл, в котором для каждой леммы указывается список документов (unloading_N), в которых она встречается в каталоге links.
    - lemma: unloading_1 unloading_2 ... unloading_N

boolean_search.py: код булева запроса (поддерживается AND, OR, NOT, скобки). Взаимодействие с пользователем через консоль ввода  

Примеры ввода:
> (взрыв OR вспышка) AND (галактика OR вселенная)

> вода AND NOT газ

> (галактика AND газ) AND NOT вспышка

#### Инструменты
- Python 3.10 (pymorphy не работает на Python 3.12)
- nltk 3.9.1 - для токенизации и работы со стоп-словами
- pymorphy2 0.9.1 - морфологический анализатор русского языка, позволяет определять леммы

#### Запуск 
- Установите необходимые пакеты: `pip install nltk pymorphy2`
- Скачайте необходимые данные NLTK (стоп-слова, punkt), если ещё не скачивали:
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```
- `python boolean_search.py`

### Задание №4

#### Требования

Для каждого скачанного html документа:
- Подсчитать tf каждого термина
- Подсчитать idf для термина
- Подсчитать tf для каждой лемматизированной формы как отношение сумм вхождения числа терминов к общему количеству терминов в документе
- Подсчитать idf для каждой лемматизированной формы

#### Входные данные

- папка links, в которой лежат HTML-файлы вида unloading_N.html, где N — целое число
- папка tokens, в которой лежат TXT-файлы со списком терминов вида tokens_unloading_N.txt для соответствующего HTML-файла в формате: <термин><\n>
- папка lemmas, в которой лежат TXT-файлы со списком лемм вида lemmas_unloading_N.txt для соответствующего HTML-файла в формате: <лемма><пробел><термин 1><пробел><термин 2>.....<пробел><терминN><\n>

#### Результат
Для каждого набора unloading_N.html, tokens_unloading_N.txt, lemmas_unloading_N.txt создаются два файла:
- unloading_N_tokens_tf_idf.txt: cписок токенов с подсчитанным tf-idf
- unloading_N_lemmas_tf_idf.txt: cписок лемм с подсчитанным tf-idf

#### Инструменты
- Python 3.10

#### Запуск
- `python tf_idf.py`


### Задание №5

#### Требования 

- Разработать поисковую систему на основе векторного поиска по построенному индексу

#### Входные данные

- папка lemmas_tf_idf, в которой лежат unloading_N_lemmas_tf_idf.txt: cписок лемм с подсчитанным tf-idf
- папка tokens_tf_idf, в которой лежат unloading_N_tokens_tf_idf.txt: cписок токенов с подсчитанным tf-idf

#### Результат 
- Для каждого поискового запроса --query, введённого пользователем, программа выводит топ-10 наиболее релевантных документов на основе косинусного сходства с вектором запроса. 
> Формат вывода: 1. <имя_документа_N>: <оценка_релевантности>

Пример ввода для лемм (--mode lemmas и default):
`python vector_search.py --query авторы`  

Пример вывода:
> Результат поиска:
> 1. unloading_40: 0.0456
> 2. unloading_44: 0.0431
> 3. unloading_92: 0.0394
> 4. unloading_59: 0.0328
> 5. unloading_82: 0.0326
> 6. unloading_60: 0.0307
> 7. unloading_19: 0.0299
> 8. unloading_14: 0.0268
> 9. unloading_43: 0.0267
> 10. unloading_98: 0.0264

Пример ввода для токенов(--mode tokens)
`python vector_search.py --query авторы --mode tokens`

Пример вывода:
> Результат поиска:
>1. unloading_44: 0.0293
>2. unloading_65: 0.0238
>3. unloading_59: 0.0220
>4. unloading_19: 0.0205
>5. unloading_49: 0.0201
>6. unloading_82: 0.0196
>7. unloading_60: 0.0193
>8. unloading_40: 0.0186
>9. unloading_32: 0.0185
>10. unloading_14: 0.0182

#### Инструменты
- Python 3.10 (pymorphy не работает на Python 3.12)
- nltk 3.9.1 - для токенизации и работы со стоп-словами
- pymorphy2 0.9.1 - морфологический анализатор русского языка, позволяет определять леммы

#### Запуск 
- Установите необходимые пакеты: `pip install nltk pymorphy2`
- Скачайте необходимые данные NLTK (стоп-слова, punkt), если ещё не скачивали:
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```


